{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24548/1202438858.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from json import dumps, loads\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(base_url, categories, label='images'):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join('output', label, urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    main_tag = soup.find('main', class_='main-content')\n",
    "    info_headers = main_tag.find_all('header', class_='info-header')\n",
    "    articles_links = []\n",
    "\n",
    "    for header in info_headers:\n",
    "        a_tag = header.find(class_='title').find('a')['href']\n",
    "        link = a_tag if a_tag.startswith('https') else f'https://www.foxnews.com{a_tag}'\n",
    "        articles_links.append(link)\n",
    "    \n",
    "    return articles_links\n",
    "\n",
    "def article_scrapper(url):\n",
    "    articles_links = get_articles_links(url)\n",
    "    data = []\n",
    "\n",
    "    def helper_scrapper(url):   \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        try:\n",
    "            time = datetime.strptime(soup.find('time').text.split('EDT')[-2].strip(), \"%B %d, %Y %I:%M%p\")\n",
    "            # time = soup.find('time').text.split('EDT')[-2].strip()\n",
    "            headline = soup.find('h1').text\n",
    "            content = soup.find('div', class_='article-body')\n",
    "            images = [(img['alt'], img['src']) for img in content.find_all('img')]\n",
    "\n",
    "            return headline, time, images\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    for url in articles_links:   \n",
    "        result = helper_scrapper(url)\n",
    "        if result is not None:\n",
    "            data.append(result)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_articles(data, n=10):\n",
    "    seen_headlines = set()\n",
    "    unique_data = []\n",
    "\n",
    "    for record in data:\n",
    "        headline = record[0]\n",
    "        if headline not in seen_headlines:\n",
    "            seen_headlines.add(headline)\n",
    "            unique_data.append(record)\n",
    "\n",
    "    return sorted(unique_data, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if not img_url.startswith('data:'):\n",
    "            response = requests.get(img_url)\n",
    "            img_data = response.content\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            width, height = img.size\n",
    "\n",
    "            # Only save images larger than 100x100 pixels\n",
    "            if width >= 100 and height >= 100:\n",
    "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def download_images(category_url, save_dir, data):\n",
    "\n",
    "    with open(os.path.join(save_dir, 'labels.csv'), 'w') as f:\n",
    "        f.write('image number,alt,article_heading\\n')\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    # parallising the downloads to make it faster\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        headlines = []\n",
    "        for x, tuple in enumerate(data):\n",
    "            headline, _, images_list = tuple\n",
    "            for i, img in enumerate(images_list):\n",
    "                alt_txt, img_url = img\n",
    "                if alt_txt.startswith('Fox News'):\n",
    "                    continue\n",
    "                if img_url and not img_url.startswith('data:'):\n",
    "                    img_url = urljoin(category_url, img_url)\n",
    "                    combined_str = f\"{alt_txt}{headline}\".encode()\n",
    "                    img_name = f'image_{x+1}_{i+1}.jpg'\n",
    "                    records.append(f'{img_name},{alt_txt.replace(\",\", \"\")},{headline.replace(\",\", \"\")}\\n')\n",
    "                    futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'labels.csv'), 'a') as f:\n",
    "            f.writelines(records)\n",
    "            \n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    (\"Crime\", \"https://www.foxnews.com/category/us/crime\"),\n",
    "]\n",
    "\n",
    "\n",
    "base_url = 'https://www.foxnews.com/'\n",
    "\n",
    "base_dir = create_directories(base_url, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images for every category: 100%|██████████| 71/71 [05:29<00:00,  4.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for category, category_url in tqdm(categories, desc='Downloading images for every category'):\n",
    "    try:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        data = article_scrapper(f'{base_url}{category}')\n",
    "        download_images(category_url, category_dir, get_latest_articles(data))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for cat in os.listdir('output/images/www.foxnews.com'):\n",
    "    if len(os.listdir(f'output/images/www.foxnews.com/{cat}')) == 1:\n",
    "        shutil.rmtree(f'output/images/www.foxnews.com/{cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "n, _ = df.shape\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((list(df.iloc[i]), list(df.iloc[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        try:\n",
    "            df = pd.read_csv(f'{base_dir}{category}/labels.csv')\n",
    "            n, _ = df.shape\n",
    "\n",
    "            pairs = []\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    article_1 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[i])[0]).group())\n",
    "                    article_2 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[j])[0]).group())\n",
    "                    if article_1 != article_2:\n",
    "                        pairs.append((list(df.iloc[i]), list(df.iloc[j])))\n",
    "\n",
    "            with open(f'{base_dir.split(\"/\")[2]}_pairs_{category}.csv', 'w') as f:\n",
    "                for i, pair in enumerate(pairs):\n",
    "                    p1, p2 = pair\n",
    "                    img1, alt1, headline1 = p1\n",
    "                    img2, alt2, headline2 = p2\n",
    "                    f.write(f'{i+1},{headline1},{headline2}\\n')\n",
    "                    f.write(f',https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.foxnews.com/{urllib.parse.quote(category)}/{img1},https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.foxnews.com/{urllib.parse.quote(category)}/{img2}\\n')\n",
    "                    f.write(f',{alt1},{alt2}\\n')\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "create_file('output/images/www.foxnews.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.startswith('www.foxnews.com_pairs'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            num_pairs.append((int(list(df.iloc[-3])[0]), file))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(460, 'www.foxnews.com_pairs_Golf.csv'),\n",
       " (607, 'www.foxnews.com_pairs_Immigration.csv'),\n",
       " (621, 'www.foxnews.com_pairs_Education.csv'),\n",
       " (637, 'www.foxnews.com_pairs_Executive.csv'),\n",
       " (663, 'www.foxnews.com_pairs_Economy.csv'),\n",
       " (664, 'www.foxnews.com_pairs_Disasters.csv'),\n",
       " (722, 'www.foxnews.com_pairs_Environment.csv'),\n",
       " (785, 'www.foxnews.com_pairs_Crime.csv'),\n",
       " (906, 'www.foxnews.com_pairs_House.csv'),\n",
       " (990, 'www.foxnews.com_pairs_Lifestyle.csv'),\n",
       " (1121, 'www.foxnews.com_pairs_Faith.csv')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_array = sorted(num_pairs, key=lambda x: x[0])\n",
    "sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "cat = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.startswith('www.foxnews.com') and file.endswith('.csv'):\n",
    "        cat.append(file.replace('.csv', '').split('_')[-1])\n",
    "\n",
    "for dir in os.listdir('output/images/www.foxnews.com'):\n",
    "    if dir not in cat:\n",
    "        shutil.rmtree(f'output/images/www.foxnews.com/{dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
