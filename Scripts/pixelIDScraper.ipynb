{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from requests_html import AsyncHTMLSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to download all versions of a pixel source code given its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a list of all cdx records of a website (also saves them in a text file)\n",
    "def basicGetCdxRecords(url, filename): #works best for small scale queries only\n",
    "    base_url = \"https://web.archive.org/cdx/search/cdx\" # Base URL of the CDX Server API\n",
    "    params = {\n",
    "        'url': url,  # URL to fetch\n",
    "        'output': 'json',      # Output format\n",
    "        'matchType': 'prefix'  # Match URLs that start with this prefix\n",
    "        #no limit variable to fetch as many versions available\n",
    "    }\n",
    "\n",
    "\n",
    "    # GET request to the API\n",
    "    response = requests.get(base_url, params=params) # each esponse contains the following information: [\"urlkey\",\"timestamp\",\"original\",\"mimetype\",\"statuscode\",\"digest\",\"length\"]\n",
    "\n",
    "    allCdxRecords = []\n",
    "    # Checks whether the requeust was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Parse the JSON response\n",
    "        with open(filename,'w') as f:\n",
    "            for record in data:\n",
    "                f.write(f\"{record}\\n\")\n",
    "                allCdxRecords.append(record)\n",
    "                # Each record represents an archived version\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "    return allCdxRecords\n",
    "\n",
    "#Returns a list of all cdx records of a website, saves them in a textfile named according to the filename, and maintaint track of the progress of the progress so far in the progress_file to continue fetching records from where they were left\n",
    "def getCdxRecords(url, filename, progress_file): #works for large-scale queries\n",
    "    base_url = \"https://web.archive.org/cdx/search/cdx\"\n",
    "    limit = 100000  # Limit per request\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'output': 'json',\n",
    "        'matchType': 'prefix',\n",
    "        'limit': limit,\n",
    "        'showResumeKey': True, #The last entry returned is the resume key, which is then used to begin fetching records exactly from where they were left\n",
    "        # 'pageSize': 1  # Smallest page size\n",
    "    }\n",
    "\n",
    "    # Load progress from the progress file\n",
    "    resume_key = None\n",
    "    if os.path.exists(progress_file): #checking the progress file to see whether a resume key exists to continue progress from\n",
    "        with open(progress_file, 'r') as f:\n",
    "            resume_key = f.read().strip()\n",
    "    \n",
    "    all_cdx_records = []\n",
    "    while True:\n",
    "        if resume_key:\n",
    "            params['resumeKey'] = resume_key #updating the resume key\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:\n",
    "                break\n",
    "            # print(\"Fetched Data: \",data)\n",
    "            \n",
    "            # Write the records to the file and append to the list\n",
    "            with open(filename, 'a') as f:\n",
    "                for record in data[:-1]:  # Last item may be the resume key\n",
    "                    f.write(f\"{record}\\n\")\n",
    "                    all_cdx_records.append(record)\n",
    "\n",
    "            # Update the resume key and save it to the progress file\n",
    "            resume_key = data[-1]\n",
    "            print(f\"Successfully fetched: {len(data)} records. Resume key: {resume_key}\")\n",
    "            with open(progress_file, 'w') as f:\n",
    "                f.write(resume_key[0])\n",
    "            \n",
    "            # If no more results are available, exit the loop\n",
    "            if 'resumeKey' not in params or not resume_key:\n",
    "                break\n",
    "            time.sleep(3) #To avoid sending too many requests to the server which then ends up refusing the connection\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "\n",
    "    return all_cdx_records\n",
    "\n",
    "# #downloads all webpages inside the all_archives_versions folder \n",
    "def downloadArchivedVersions(fileWithRecords, archivedDirectory): #fileWithRecords is the name of the text file with all cdx records to download, archivedDirectory is the name of directory where to save all the web pages.\n",
    "\n",
    "    wayback_base_url = \"https://web.archive.org/web/\"\n",
    "    save_dir = archivedDirectory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(fileWithRecords, 'r') as f:\n",
    "        records = f.readlines()\n",
    "\n",
    "    for record in records:\n",
    "        record = eval(record.strip())  # Convert the string back to a list\n",
    "        timestamp = record[1]\n",
    "        original_url = record[2]\n",
    "\n",
    "        wayback_url = f\"{wayback_base_url}{timestamp}/{original_url}\" #A resource at the wayback has a url of this format\n",
    "        filename = f\"{timestamp}.html\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Check if file already exists\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"File already exists: {filepath}\") #to continue from saved progress\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(wayback_url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(response.text)\n",
    "                    print(f\"Downloaded and saved: {filepath}\")\n",
    "                    break\n",
    "                elif response.status_code == 404:\n",
    "                    print(f\"File not found (404): {wayback_url}. Skipping...\")\n",
    "                    break  # Stop retrying on 404 errors since it just doesn't exist\n",
    "                else:\n",
    "                    print(f\"Failed to download {wayback_url}: {response.status_code}\")\n",
    "                    time.sleep(5) #wait before retrying\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {wayback_url}: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "url = \"https://connect.facebook.net/signals/config/\" #the url of the meta pixel\n",
    "filename = \"allPixelRecords.txt\"\n",
    "progress_file = \"progress.txt\"\n",
    "cdx_records = getCdxRecords(url, filename, progress_file)\n",
    "\n",
    "# downloadArchivedVersions(filename,'all_archived_versions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website Name</th>\n",
       "      <th>Pixel ID</th>\n",
       "      <th>Fetched Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://onepathnetwork.com/?gad_source=1&amp;gclid...</td>\n",
       "      <td>128726134153194</td>\n",
       "      <td>20240913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://react-portfolio-alpha-nine-57.vercel.app</td>\n",
       "      <td>25826907853621873</td>\n",
       "      <td>20240913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Website Name           Pixel ID  \\\n",
       "0  https://onepathnetwork.com/?gad_source=1&gclid...    128726134153194   \n",
       "1   https://react-portfolio-alpha-nine-57.vercel.app  25826907853621873   \n",
       "\n",
       "  Fetched Date  \n",
       "0     20240913  \n",
       "1     20240913  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pixel IDs from Tranc's top 10k websites. Can look at snapshots to find the ID as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the latest top 1 million websites from Tranco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# Replace with your Tranco credentials\n",
    "username = os.getenv('USERNAME')\n",
    "api_token = os.getenv('TRANCO_TOKEN')\n",
    "\n",
    "def get_latest_list_metadata():\n",
    "    url = 'https://tranco-list.eu/api/lists/date/latest'\n",
    "    try:\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(username, api_token))\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data['available']:\n",
    "            return data['download']\n",
    "        else:\n",
    "            print(\"No list available at the moment.\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching latest list metadata: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_list(download_url):\n",
    "    try:\n",
    "        response = requests.get(download_url, auth=HTTPBasicAuth(username, api_token))\n",
    "        response.raise_for_status()\n",
    "        with open('tranco_top_10k.csv', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Tranco Top 10k list downloaded successfully.\")\n",
    "        # Load into DataFrame\n",
    "        df = pd.read_csv('tranco_top_10k.csv', header=None, names=['Rank', 'Domain'])\n",
    "        return df\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading Tranco list: {e}\")\n",
    "        return None\n",
    "\n",
    "download_url = get_latest_list_metadata()\n",
    "\n",
    "# If a valid URL is found, download the list\n",
    "if download_url:\n",
    "    df = download_list(download_url)\n",
    "    if df is not None:\n",
    "        print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel ID fetched directly:  None\n",
      "Attempt 1: Getting snapshot for 20240914\n",
      "Check:  {'url': 'idhfsin.com', 'archived_snapshots': {}, 'timestamp': '20240914'}  for  20240914\n",
      "Processed 1/1: idhfsin.com (Pixel ID: None)\n",
      "  Website Name              Pixel ID    Fetched Date\n",
      "0  riteaid.com  ['1264059003707256']  20240910210119\n",
      "1     bruh.com                   NaN        20240914\n",
      "2  idhfsin.com                  None        20240914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getPixelID(website, driver, max_retries=5, delay=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} to fetch Pixel ID from {website}\")\n",
    "\n",
    "            time.sleep(4.5)\n",
    "            if not website.startswith(('http://', 'https://')): #Adding the protocol since the tranco list does not contain the schema in the urls\n",
    "                try:\n",
    "                    driver.get('https://' + website)\n",
    "                except:\n",
    "                    driver.get('http://' + website)\n",
    "                \n",
    "            else:\n",
    "                driver.get(website)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            scripts = soup.find_all('script')\n",
    "\n",
    "            pattern = re.compile(r'connect\\.facebook\\.net/signals/config/(\\d+)')\n",
    "\n",
    "            all_pixels = []\n",
    "            \n",
    "            for script in scripts:\n",
    "                if script.has_attr('src'):\n",
    "                    src = script['src']\n",
    "                    if \"connect.facebook.net/signals/config\" in src:\n",
    "                        match = pattern.search(src)\n",
    "                        if match:\n",
    "                            print(f\"Pixel ID found: {match.group(1)}\")\n",
    "                            all_pixels.append(match.group(1))\n",
    "            \n",
    "            if len(all_pixels)==0:\n",
    "                print(f\"No Pixel ID found on {website}\")\n",
    "                return None\n",
    "            else:\n",
    "                return all_pixels\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Pixel ID from {website}: {e}\")\n",
    "            print(f\"Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    print(f\"Failed to fetch Pixel ID from {website} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def get_wayback_snapshot(website, date, max_retries=5):\n",
    "    url = f\"http://archive.org/wayback/available?url={website}&timestamp={date}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}: Getting snapshot for {date}\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            print(\"Check: \",data, \" for \",date)\n",
    "            if 'archived_snapshots' in data and data['archived_snapshots']:\n",
    "                return (data['archived_snapshots']['closest']['url'],data['archived_snapshots']['closest']['timestamp'])\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(5)\n",
    "    print(f\"Failed to get snapshot for {date} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def check_past_versions(website, driver):\n",
    "    current_date = datetime.now()\n",
    "    for i in range(1): #set to 60 later for going back every month for 5 years\n",
    "        past_date = (current_date - timedelta(days=i*600)).strftime('%Y%m%d') #set days i*30 for 1 month\n",
    "        snapshot_url = get_wayback_snapshot(website, past_date)\n",
    "        if snapshot_url:\n",
    "            print(f\"Searching snapshot: {snapshot_url[0]}\")\n",
    "            pixelID = getPixelID(snapshot_url[0], driver)\n",
    "            if pixelID:\n",
    "                print(f\"Found pixel ID: {pixelID} at the snapsot: {snapshot_url[1]} \")\n",
    "                return pixelID, snapshot_url[1]\n",
    "    return None, None\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Website Name\", \"Pixel ID\", \"Fetched Date\"])\n",
    "\n",
    "def save_progress(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def populateDataframe(urls, progress_file):\n",
    "    # Load previous progress if any\n",
    "    progress_df = load_progress(progress_file)\n",
    "    completed_urls = progress_df['Website Name'].tolist()\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    for idx, website in enumerate(urls):\n",
    "        if website in completed_urls:\n",
    "            print(f\"Skipping {website}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        # Process the URL\n",
    "        pixelID = getPixelID(website, driver)\n",
    "        print(\"Pixel ID fetched directly: \",pixelID)\n",
    "        fetch_date = datetime.now().strftime('%Y%m%d')  # Use current date if found in current version\n",
    "\n",
    "        if not pixelID:  # If no pixel ID found, check Wayback Machine\n",
    "            pixelID, past_date = check_past_versions(website, driver)\n",
    "            fetch_date = past_date if past_date else fetch_date  # Use past date if found in Wayback Machine\n",
    "        \n",
    "        # Append the result to the DataFrame\n",
    "        new_record = {\"Website Name\": website, \"Pixel ID\": pixelID, \"Fetched Date\": fetch_date}\n",
    "        progress_df = pd.concat([progress_df, pd.DataFrame([new_record])], ignore_index=True)\n",
    "\n",
    "        # Save progress after each URL is processed\n",
    "        save_progress(progress_df, progress_file)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed {idx + 1}/{total_urls}: {website} (Pixel ID: {pixelID})\")\n",
    "\n",
    "    driver.quit()\n",
    "    return progress_df\n",
    "\n",
    "\n",
    "progress_file = 'scraping_progress.csv'\n",
    "# urls = pd.read_csv('tranco_top_10k.csv')['website'].to_list()\n",
    "urls = ['idhfsin.com']\n",
    "\n",
    "df = populateDataframe(urls, progress_file)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler only but using archive.org API to fetch valid snapshots (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping google.com, already processed.\n",
      "Skipping amazonaws.com, already processed.\n",
      "Skipping microsoft.com, already processed.\n",
      "Skipping facebook.com, already processed.\n",
      "Attempt 1: Getting snapshot for 20240914\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240914'}  for  20240914\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240815\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240815'}  for  20240815\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240716\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240716'}  for  20240716\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240616\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240616'}  for  20240616\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240517\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240517'}  for  20240517\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c55e90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240417\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240417'}  for  20240417\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240318\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240318'}  for  20240318\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20240217\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20240217'}  for  20240217\n",
      "No snapshot available akamai.net : 20240217\n",
      "Attempt 1: Getting snapshot for 20240118\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20240118'}  for  20240118\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20231219\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20231219'}  for  20231219\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20231119\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20231119'}  for  20231119\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c59e90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20231020\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20231020'}  for  20231020\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230920\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230920'}  for  20230920\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230821\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230821'}  for  20230821\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230722\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20230722'}  for  20230722\n",
      "No snapshot available akamai.net : 20230722\n",
      "Attempt 1: Getting snapshot for 20230622\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20230622'}  for  20230622\n",
      "No snapshot available akamai.net : 20230622\n",
      "Attempt 1: Getting snapshot for 20230523\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230523'}  for  20230523\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230423\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230423'}  for  20230423\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c54150>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c642d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c65e90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 4: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c67b90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 5: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230324\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20230324'}  for  20230324\n",
      "No snapshot available akamai.net : 20230324\n",
      "Attempt 1: Getting snapshot for 20230222\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230222'}  for  20230222\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c5a8d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20230123\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20230123'}  for  20230123\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20221224\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20221224'}  for  20221224\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c70710>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20221124\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20221124'}  for  20221124\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c644d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20221025\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20221025'}  for  20221025\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c65fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220925\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220925'}  for  20220925\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220826\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220826'}  for  20220826\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220727\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220727'}  for  20220727\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220627\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220627'}  for  20220627\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220528\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220528'}  for  20220528\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220428\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220428'}  for  20220428\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220329\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20220329'}  for  20220329\n",
      "No snapshot available akamai.net : 20220329\n",
      "Attempt 1: Getting snapshot for 20220227\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220227'}  for  20220227\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20220128\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20220128'}  for  20220128\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20211229\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20211229'}  for  20211229\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20211129\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20211129'}  for  20211129\n",
      "No snapshot available akamai.net : 20211129\n",
      "Attempt 1: Getting snapshot for 20211030\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20211030'}  for  20211030\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210930\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20210930'}  for  20210930\n",
      "No snapshot available akamai.net : 20210930\n",
      "Attempt 1: Getting snapshot for 20210831\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210831'}  for  20210831\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210801\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210801'}  for  20210801\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210702\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210702'}  for  20210702\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210602\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210602'}  for  20210602\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210503\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210503'}  for  20210503\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210403\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20210403'}  for  20210403\n",
      "No snapshot available akamai.net : 20210403\n",
      "Attempt 1: Getting snapshot for 20210304\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210304'}  for  20210304\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210202\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20210202'}  for  20210202\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c5bcd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20210103\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20210103'}  for  20210103\n",
      "No snapshot available akamai.net : 20210103\n",
      "Attempt 1: Getting snapshot for 20201204\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20201204'}  for  20201204\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20201104\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20201104'}  for  20201104\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c5bc90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20201005\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20201005'}  for  20201005\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c69710>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200905\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200905'}  for  20200905\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200806\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200806'}  for  20200806\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200707\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200707'}  for  20200707\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200607\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200607'}  for  20200607\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c74d90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200508\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200508'}  for  20200508\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c75090>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c6ce90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200408\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200408'}  for  20200408\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200309\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200309'}  for  20200309\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200208\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20200208'}  for  20200208\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c76f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c66dd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c64710>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 4: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20200109\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {}, 'timestamp': '20200109'}  for  20200109\n",
      "No snapshot available akamai.net : 20200109\n",
      "Attempt 1: Getting snapshot for 20191210\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20191210'}  for  20191210\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Attempt 1: Getting snapshot for 20191110\n",
      "Check:  {'url': 'akamai.net', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/19990208011229/http://www.akamai.net:80/', 'timestamp': '19990208011229'}}, 'timestamp': '20191110'}  for  20191110\n",
      "Downloading snapshot: http://web.archive.org/web/19990208011229/http://www.akamai.net:80/\n",
      "Attempt 1: Getting snapshot for 19990208011229\n",
      "Error downloading http://web.archive.org/web/19990208011229/http://www.akamai.net:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/19990208011229/http://www.akamai.net:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c731d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 19990208011229\n",
      "Downloaded and saved: akamai.net/19990208011229.html\n",
      "Processed 5/10000: akamai.net\n",
      "Attempt 1: Getting snapshot for 20240914\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240914'}  for  20240914\n",
      "No snapshot available a-msedge.net : 20240914\n",
      "Attempt 1: Getting snapshot for 20240815\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240815'}  for  20240815\n",
      "No snapshot available a-msedge.net : 20240815\n",
      "Attempt 1: Getting snapshot for 20240716\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240716'}  for  20240716\n",
      "No snapshot available a-msedge.net : 20240716\n",
      "Attempt 1: Getting snapshot for 20240616\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240616'}  for  20240616\n",
      "No snapshot available a-msedge.net : 20240616\n",
      "Attempt 1: Getting snapshot for 20240517\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240517'}  for  20240517\n",
      "No snapshot available a-msedge.net : 20240517\n",
      "Attempt 1: Getting snapshot for 20240417\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240417'}  for  20240417\n",
      "No snapshot available a-msedge.net : 20240417\n",
      "Attempt 1: Getting snapshot for 20240318\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240318'}  for  20240318\n",
      "No snapshot available a-msedge.net : 20240318\n",
      "Attempt 1: Getting snapshot for 20240217\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240217'}  for  20240217\n",
      "No snapshot available a-msedge.net : 20240217\n",
      "Attempt 1: Getting snapshot for 20240118\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20240118'}  for  20240118\n",
      "No snapshot available a-msedge.net : 20240118\n",
      "Attempt 1: Getting snapshot for 20231219\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20231219'}  for  20231219\n",
      "No snapshot available a-msedge.net : 20231219\n",
      "Attempt 1: Getting snapshot for 20231119\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20231119'}  for  20231119\n",
      "No snapshot available a-msedge.net : 20231119\n",
      "Attempt 1: Getting snapshot for 20231020\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20231020'}  for  20231020\n",
      "No snapshot available a-msedge.net : 20231020\n",
      "Attempt 1: Getting snapshot for 20230920\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230920'}  for  20230920\n",
      "No snapshot available a-msedge.net : 20230920\n",
      "Attempt 1: Getting snapshot for 20230821\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230821'}  for  20230821\n",
      "No snapshot available a-msedge.net : 20230821\n",
      "Attempt 1: Getting snapshot for 20230722\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230722'}  for  20230722\n",
      "No snapshot available a-msedge.net : 20230722\n",
      "Attempt 1: Getting snapshot for 20230622\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230622'}  for  20230622\n",
      "No snapshot available a-msedge.net : 20230622\n",
      "Attempt 1: Getting snapshot for 20230523\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230523'}  for  20230523\n",
      "No snapshot available a-msedge.net : 20230523\n",
      "Attempt 1: Getting snapshot for 20230423\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230423'}  for  20230423\n",
      "No snapshot available a-msedge.net : 20230423\n",
      "Attempt 1: Getting snapshot for 20230324\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230324'}  for  20230324\n",
      "No snapshot available a-msedge.net : 20230324\n",
      "Attempt 1: Getting snapshot for 20230222\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230222'}  for  20230222\n",
      "No snapshot available a-msedge.net : 20230222\n",
      "Attempt 1: Getting snapshot for 20230123\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20230123'}  for  20230123\n",
      "No snapshot available a-msedge.net : 20230123\n",
      "Attempt 1: Getting snapshot for 20221224\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20221224'}  for  20221224\n",
      "No snapshot available a-msedge.net : 20221224\n",
      "Attempt 1: Getting snapshot for 20221124\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20221124'}  for  20221124\n",
      "No snapshot available a-msedge.net : 20221124\n",
      "Attempt 1: Getting snapshot for 20221025\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20221025'}  for  20221025\n",
      "No snapshot available a-msedge.net : 20221025\n",
      "Attempt 1: Getting snapshot for 20220925\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220925'}  for  20220925\n",
      "No snapshot available a-msedge.net : 20220925\n",
      "Attempt 1: Getting snapshot for 20220826\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220826'}  for  20220826\n",
      "No snapshot available a-msedge.net : 20220826\n",
      "Attempt 1: Getting snapshot for 20220727\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220727'}  for  20220727\n",
      "No snapshot available a-msedge.net : 20220727\n",
      "Attempt 1: Getting snapshot for 20220627\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220627'}  for  20220627\n",
      "No snapshot available a-msedge.net : 20220627\n",
      "Attempt 1: Getting snapshot for 20220528\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220528'}  for  20220528\n",
      "No snapshot available a-msedge.net : 20220528\n",
      "Attempt 1: Getting snapshot for 20220428\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220428'}  for  20220428\n",
      "No snapshot available a-msedge.net : 20220428\n",
      "Attempt 1: Getting snapshot for 20220329\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220329'}  for  20220329\n",
      "No snapshot available a-msedge.net : 20220329\n",
      "Attempt 1: Getting snapshot for 20220227\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220227'}  for  20220227\n",
      "No snapshot available a-msedge.net : 20220227\n",
      "Attempt 1: Getting snapshot for 20220128\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20220128'}  for  20220128\n",
      "No snapshot available a-msedge.net : 20220128\n",
      "Attempt 1: Getting snapshot for 20211229\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20211229'}  for  20211229\n",
      "No snapshot available a-msedge.net : 20211229\n",
      "Attempt 1: Getting snapshot for 20211129\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20211129'}  for  20211129\n",
      "No snapshot available a-msedge.net : 20211129\n",
      "Attempt 1: Getting snapshot for 20211030\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20211030'}  for  20211030\n",
      "No snapshot available a-msedge.net : 20211030\n",
      "Attempt 1: Getting snapshot for 20210930\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210930'}  for  20210930\n",
      "No snapshot available a-msedge.net : 20210930\n",
      "Attempt 1: Getting snapshot for 20210831\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210831'}  for  20210831\n",
      "No snapshot available a-msedge.net : 20210831\n",
      "Attempt 1: Getting snapshot for 20210801\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210801'}  for  20210801\n",
      "No snapshot available a-msedge.net : 20210801\n",
      "Attempt 1: Getting snapshot for 20210702\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210702'}  for  20210702\n",
      "No snapshot available a-msedge.net : 20210702\n",
      "Attempt 1: Getting snapshot for 20210602\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210602'}  for  20210602\n",
      "No snapshot available a-msedge.net : 20210602\n",
      "Attempt 1: Getting snapshot for 20210503\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210503'}  for  20210503\n",
      "No snapshot available a-msedge.net : 20210503\n",
      "Attempt 1: Getting snapshot for 20210403\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210403'}  for  20210403\n",
      "No snapshot available a-msedge.net : 20210403\n",
      "Attempt 1: Getting snapshot for 20210304\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210304'}  for  20210304\n",
      "No snapshot available a-msedge.net : 20210304\n",
      "Attempt 1: Getting snapshot for 20210202\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210202'}  for  20210202\n",
      "No snapshot available a-msedge.net : 20210202\n",
      "Attempt 1: Getting snapshot for 20210103\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20210103'}  for  20210103\n",
      "No snapshot available a-msedge.net : 20210103\n",
      "Attempt 1: Getting snapshot for 20201204\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20201204'}  for  20201204\n",
      "No snapshot available a-msedge.net : 20201204\n",
      "Attempt 1: Getting snapshot for 20201104\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20201104'}  for  20201104\n",
      "No snapshot available a-msedge.net : 20201104\n",
      "Attempt 1: Getting snapshot for 20201005\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20201005'}  for  20201005\n",
      "No snapshot available a-msedge.net : 20201005\n",
      "Attempt 1: Getting snapshot for 20200905\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200905'}  for  20200905\n",
      "No snapshot available a-msedge.net : 20200905\n",
      "Attempt 1: Getting snapshot for 20200806\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200806'}  for  20200806\n",
      "No snapshot available a-msedge.net : 20200806\n",
      "Attempt 1: Getting snapshot for 20200707\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200707'}  for  20200707\n",
      "No snapshot available a-msedge.net : 20200707\n",
      "Attempt 1: Getting snapshot for 20200607\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200607'}  for  20200607\n",
      "No snapshot available a-msedge.net : 20200607\n",
      "Attempt 1: Getting snapshot for 20200508\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200508'}  for  20200508\n",
      "No snapshot available a-msedge.net : 20200508\n",
      "Attempt 1: Getting snapshot for 20200408\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200408'}  for  20200408\n",
      "No snapshot available a-msedge.net : 20200408\n",
      "Attempt 1: Getting snapshot for 20200309\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200309'}  for  20200309\n",
      "No snapshot available a-msedge.net : 20200309\n",
      "Attempt 1: Getting snapshot for 20200208\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200208'}  for  20200208\n",
      "No snapshot available a-msedge.net : 20200208\n",
      "Attempt 1: Getting snapshot for 20200109\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20200109'}  for  20200109\n",
      "No snapshot available a-msedge.net : 20200109\n",
      "Attempt 1: Getting snapshot for 20191210\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20191210'}  for  20191210\n",
      "No snapshot available a-msedge.net : 20191210\n",
      "Attempt 1: Getting snapshot for 20191110\n",
      "Check:  {'url': 'a-msedge.net', 'archived_snapshots': {}, 'timestamp': '20191110'}  for  20191110\n",
      "No snapshot available a-msedge.net : 20191110\n",
      "Processed 6/10000: a-msedge.net\n",
      "Attempt 1: Getting snapshot for 20240914\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240914'}  for  20240914\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240815\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240815'}  for  20240815\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240716\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20240716'}  for  20240716\n",
      "No snapshot available googleapis.com : 20240716\n",
      "Attempt 1: Getting snapshot for 20240616\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240616'}  for  20240616\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240517\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20240517'}  for  20240517\n",
      "No snapshot available googleapis.com : 20240517\n",
      "Attempt 1: Getting snapshot for 20240417\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240417'}  for  20240417\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240318\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240318'}  for  20240318\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240217\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240217'}  for  20240217\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20240118\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20240118'}  for  20240118\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20231219\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20231219'}  for  20231219\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20231119\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20231119'}  for  20231119\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20231020\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20231020'}  for  20231020\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230920\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230920'}  for  20230920\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230821\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230821'}  for  20230821\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230722\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230722'}  for  20230722\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230622\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20230622'}  for  20230622\n",
      "No snapshot available googleapis.com : 20230622\n",
      "Attempt 1: Getting snapshot for 20230523\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230523'}  for  20230523\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230423\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230423'}  for  20230423\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c587d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c70790>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c67c10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 4: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c66b90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 5: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c775d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 6: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c76590>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 7: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c7ff50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 8: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c89b10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 9: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c73490>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 10: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c760d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 1: Getting snapshot for 20230324\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230324'}  for  20230324\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230222\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20230222'}  for  20230222\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20230123\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20230123'}  for  20230123\n",
      "No snapshot available googleapis.com : 20230123\n",
      "Attempt 1: Getting snapshot for 20221224\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20221224'}  for  20221224\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20221124\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20221124'}  for  20221124\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c3cc50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c72d90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20221025\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20221025'}  for  20221025\n",
      "No snapshot available googleapis.com : 20221025\n",
      "Attempt 1: Getting snapshot for 20220925\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20220925'}  for  20220925\n",
      "No snapshot available googleapis.com : 20220925\n",
      "Attempt 1: Getting snapshot for 20220826\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220826'}  for  20220826\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220727\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220727'}  for  20220727\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220627\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220627'}  for  20220627\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220528\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220528'}  for  20220528\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220428\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20220428'}  for  20220428\n",
      "No snapshot available googleapis.com : 20220428\n",
      "Attempt 1: Getting snapshot for 20220329\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220329'}  for  20220329\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220227\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20220227'}  for  20220227\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20220128\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20220128'}  for  20220128\n",
      "No snapshot available googleapis.com : 20220128\n",
      "Attempt 1: Getting snapshot for 20211229\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20211229'}  for  20211229\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20211129\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20211129'}  for  20211129\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20211030\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20211030'}  for  20211030\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210930\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210930'}  for  20210930\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210831\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210831'}  for  20210831\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210801\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210801'}  for  20210801\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210702\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20210702'}  for  20210702\n",
      "No snapshot available googleapis.com : 20210702\n",
      "Attempt 1: Getting snapshot for 20210602\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20210602'}  for  20210602\n",
      "No snapshot available googleapis.com : 20210602\n",
      "Attempt 1: Getting snapshot for 20210503\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210503'}  for  20210503\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210403\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210403'}  for  20210403\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210304\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20210304'}  for  20210304\n",
      "No snapshot available googleapis.com : 20210304\n",
      "Attempt 1: Getting snapshot for 20210202\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210202'}  for  20210202\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20210103\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20210103'}  for  20210103\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20201204\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20201204'}  for  20201204\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20201104\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20201104'}  for  20201104\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20201005\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20201005'}  for  20201005\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c536d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200905\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200905'}  for  20200905\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c5b910>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c94390>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13d30810>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 4: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c66f50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 5: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200806\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200806'}  for  20200806\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200707\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20200707'}  for  20200707\n",
      "No snapshot available googleapis.com : 20200707\n",
      "Attempt 1: Getting snapshot for 20200607\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200607'}  for  20200607\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200508\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {}, 'timestamp': '20200508'}  for  20200508\n",
      "No snapshot available googleapis.com : 20200508\n",
      "Attempt 1: Getting snapshot for 20200408\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200408'}  for  20200408\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200309\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200309'}  for  20200309\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200208\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200208'}  for  20200208\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20200109\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20200109'}  for  20200109\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20191210\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20191210'}  for  20191210\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Attempt 1: Getting snapshot for 20191110\n",
      "Check:  {'url': 'googleapis.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/', 'timestamp': '20100207201341'}}, 'timestamp': '20191110'}  for  20191110\n",
      "Downloading snapshot: http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/\n",
      "Attempt 1: Getting snapshot for 20100207201341\n",
      "Error downloading http://web.archive.org/web/20100207201341/http://www.googleapis.com:80/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20100207201341/http://www.googleapis.com:80/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c5bbd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20100207201341\n",
      "Downloaded and saved: googleapis.com/20100207201341.html\n",
      "Processed 7/10000: googleapis.com\n",
      "Attempt 1: Getting snapshot for 20240914\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240914022725/https://www.apple.com/', 'timestamp': '20240914022725'}}, 'timestamp': '20240914'}  for  20240914\n",
      "Downloading snapshot: http://web.archive.org/web/20240914022725/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240914022725\n",
      "Downloaded and saved: apple.com/20240914022725.html\n",
      "Attempt 1: Getting snapshot for 20240815\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {}, 'timestamp': '20240815'}  for  20240815\n",
      "No snapshot available apple.com : 20240815\n",
      "Attempt 1: Getting snapshot for 20240716\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240717000022/https://www.apple.com/', 'timestamp': '20240717000022'}}, 'timestamp': '20240716'}  for  20240716\n",
      "Downloading snapshot: http://web.archive.org/web/20240717000022/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240717000022\n",
      "Downloaded and saved: apple.com/20240717000022.html\n",
      "Attempt 1: Getting snapshot for 20240616\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {}, 'timestamp': '20240616'}  for  20240616\n",
      "No snapshot available apple.com : 20240616\n",
      "Attempt 1: Getting snapshot for 20240517\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {}, 'timestamp': '20240517'}  for  20240517\n",
      "No snapshot available apple.com : 20240517\n",
      "Attempt 1: Getting snapshot for 20240417\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240417235739/https://www.apple.com/', 'timestamp': '20240417235739'}}, 'timestamp': '20240417'}  for  20240417\n",
      "Downloading snapshot: http://web.archive.org/web/20240417235739/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240417235739\n",
      "Downloaded and saved: apple.com/20240417235739.html\n",
      "Attempt 1: Getting snapshot for 20240318\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240318235924/https://www.apple.com/', 'timestamp': '20240318235924'}}, 'timestamp': '20240318'}  for  20240318\n",
      "Downloading snapshot: http://web.archive.org/web/20240318235924/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240318235924\n",
      "Downloaded and saved: apple.com/20240318235924.html\n",
      "Attempt 1: Getting snapshot for 20240217\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240217235723/https://www.apple.com/', 'timestamp': '20240217235723'}}, 'timestamp': '20240217'}  for  20240217\n",
      "Downloading snapshot: http://web.archive.org/web/20240217235723/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240217235723\n",
      "Downloaded and saved: apple.com/20240217235723.html\n",
      "Attempt 1: Getting snapshot for 20240118\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20240118235937/https://www.apple.com/', 'timestamp': '20240118235937'}}, 'timestamp': '20240118'}  for  20240118\n",
      "Downloading snapshot: http://web.archive.org/web/20240118235937/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20240118235937\n",
      "Downloaded and saved: apple.com/20240118235937.html\n",
      "Attempt 1: Getting snapshot for 20231219\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20231219235957/https://www.apple.com/', 'timestamp': '20231219235957'}}, 'timestamp': '20231219'}  for  20231219\n",
      "Downloading snapshot: http://web.archive.org/web/20231219235957/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20231219235957\n",
      "Downloaded and saved: apple.com/20231219235957.html\n",
      "Attempt 1: Getting snapshot for 20231119\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20231120000123/http://www.apple.com/', 'timestamp': '20231120000123'}}, 'timestamp': '20231119'}  for  20231119\n",
      "Downloading snapshot: http://web.archive.org/web/20231120000123/http://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20231120000123\n",
      "Downloaded and saved: apple.com/20231120000123.html\n",
      "Attempt 1: Getting snapshot for 20231020\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20231021000052/https://www.apple.com/', 'timestamp': '20231021000052'}}, 'timestamp': '20231020'}  for  20231020\n",
      "Downloading snapshot: http://web.archive.org/web/20231021000052/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20231021000052\n",
      "Downloaded and saved: apple.com/20231021000052.html\n",
      "Attempt 1: Getting snapshot for 20230920\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230921000408/https://www.apple.com/', 'timestamp': '20230921000408'}}, 'timestamp': '20230920'}  for  20230920\n",
      "Downloading snapshot: http://web.archive.org/web/20230921000408/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230921000408\n",
      "Downloaded and saved: apple.com/20230921000408.html\n",
      "Attempt 1: Getting snapshot for 20230821\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230821235852/https://www.apple.com/', 'timestamp': '20230821235852'}}, 'timestamp': '20230821'}  for  20230821\n",
      "Downloading snapshot: http://web.archive.org/web/20230821235852/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230821235852\n",
      "Downloaded and saved: apple.com/20230821235852.html\n",
      "Attempt 1: Getting snapshot for 20230722\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230722235902/https://www.apple.com/', 'timestamp': '20230722235902'}}, 'timestamp': '20230722'}  for  20230722\n",
      "Downloading snapshot: http://web.archive.org/web/20230722235902/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230722235902\n",
      "Error downloading http://web.archive.org/web/20230722235902/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20230722235902/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c7f5d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20230722235902\n",
      "Error downloading http://web.archive.org/web/20230722235902/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20230722235902/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c95250>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 20230722235902\n",
      "Error downloading http://web.archive.org/web/20230722235902/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20230722235902/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c96cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 4: Getting snapshot for 20230722235902\n",
      "Downloaded and saved: apple.com/20230722235902.html\n",
      "Attempt 1: Getting snapshot for 20230622\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230622235828/https://www.apple.com/', 'timestamp': '20230622235828'}}, 'timestamp': '20230622'}  for  20230622\n",
      "Downloading snapshot: http://web.archive.org/web/20230622235828/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230622235828\n",
      "Downloaded and saved: apple.com/20230622235828.html\n",
      "Attempt 1: Getting snapshot for 20230523\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230524000028/https://www.apple.com/', 'timestamp': '20230524000028'}}, 'timestamp': '20230523'}  for  20230523\n",
      "Downloading snapshot: http://web.archive.org/web/20230524000028/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230524000028\n",
      "Downloaded and saved: apple.com/20230524000028.html\n",
      "Attempt 1: Getting snapshot for 20230423\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230423235039/https://www.apple.com/', 'timestamp': '20230423235039'}}, 'timestamp': '20230423'}  for  20230423\n",
      "Downloading snapshot: http://web.archive.org/web/20230423235039/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230423235039\n",
      "Downloaded and saved: apple.com/20230423235039.html\n",
      "Attempt 1: Getting snapshot for 20230324\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230325000029/https://www.apple.com/', 'timestamp': '20230325000029'}}, 'timestamp': '20230324'}  for  20230324\n",
      "Downloading snapshot: http://web.archive.org/web/20230325000029/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230325000029\n",
      "Downloaded and saved: apple.com/20230325000029.html\n",
      "Attempt 1: Getting snapshot for 20230222\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230222235144/https://www.apple.com/', 'timestamp': '20230222235144'}}, 'timestamp': '20230222'}  for  20230222\n",
      "Downloading snapshot: http://web.archive.org/web/20230222235144/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230222235144\n",
      "Downloaded and saved: apple.com/20230222235144.html\n",
      "Attempt 1: Getting snapshot for 20230123\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20230123235705/https://www.apple.com/', 'timestamp': '20230123235705'}}, 'timestamp': '20230123'}  for  20230123\n",
      "Downloading snapshot: http://web.archive.org/web/20230123235705/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20230123235705\n",
      "Error downloading http://web.archive.org/web/20230123235705/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20230123235705/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c7d150>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20230123235705\n",
      "Downloaded and saved: apple.com/20230123235705.html\n",
      "Attempt 1: Getting snapshot for 20221224\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20221224235749/https://www.apple.com/', 'timestamp': '20221224235749'}}, 'timestamp': '20221224'}  for  20221224\n",
      "Downloading snapshot: http://web.archive.org/web/20221224235749/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20221224235749\n",
      "Downloaded and saved: apple.com/20221224235749.html\n",
      "Attempt 1: Getting snapshot for 20221124\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20221124235043/https://www.apple.com/', 'timestamp': '20221124235043'}}, 'timestamp': '20221124'}  for  20221124\n",
      "Downloading snapshot: http://web.archive.org/web/20221124235043/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20221124235043\n",
      "Downloaded and saved: apple.com/20221124235043.html\n",
      "Attempt 1: Getting snapshot for 20221025\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20221026000134/https://www.apple.com/', 'timestamp': '20221026000134'}}, 'timestamp': '20221025'}  for  20221025\n",
      "Downloading snapshot: http://web.archive.org/web/20221026000134/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20221026000134\n",
      "Downloaded and saved: apple.com/20221026000134.html\n",
      "Attempt 1: Getting snapshot for 20220925\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220925235404/https://www.apple.com/', 'timestamp': '20220925235404'}}, 'timestamp': '20220925'}  for  20220925\n",
      "Downloading snapshot: http://web.archive.org/web/20220925235404/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220925235404\n",
      "Downloaded and saved: apple.com/20220925235404.html\n",
      "Attempt 1: Getting snapshot for 20220826\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220827000138/https://www.apple.com/', 'timestamp': '20220827000138'}}, 'timestamp': '20220826'}  for  20220826\n",
      "Downloading snapshot: http://web.archive.org/web/20220827000138/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220827000138\n",
      "Error downloading http://web.archive.org/web/20220827000138/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20220827000138/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c2dc10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20220827000138\n",
      "Error downloading http://web.archive.org/web/20220827000138/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20220827000138/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c3b110>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 3: Getting snapshot for 20220827000138\n",
      "Downloaded and saved: apple.com/20220827000138.html\n",
      "Attempt 1: Getting snapshot for 20220727\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220728000155/https://www.apple.com/', 'timestamp': '20220728000155'}}, 'timestamp': '20220727'}  for  20220727\n",
      "Downloading snapshot: http://web.archive.org/web/20220728000155/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220728000155\n",
      "Downloaded and saved: apple.com/20220728000155.html\n",
      "Attempt 1: Getting snapshot for 20220627\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220628000301/https://www.apple.com/', 'timestamp': '20220628000301'}}, 'timestamp': '20220627'}  for  20220627\n",
      "Downloading snapshot: http://web.archive.org/web/20220628000301/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220628000301\n",
      "Downloaded and saved: apple.com/20220628000301.html\n",
      "Attempt 1: Getting snapshot for 20220528\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {}, 'timestamp': '20220528'}  for  20220528\n",
      "No snapshot available apple.com : 20220528\n",
      "Attempt 1: Getting snapshot for 20220428\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220428235720/https://www.apple.com/', 'timestamp': '20220428235720'}}, 'timestamp': '20220428'}  for  20220428\n",
      "Downloading snapshot: http://web.archive.org/web/20220428235720/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220428235720\n",
      "Downloaded and saved: apple.com/20220428235720.html\n",
      "Attempt 1: Getting snapshot for 20220329\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220329235125/https://www.apple.com/', 'timestamp': '20220329235125'}}, 'timestamp': '20220329'}  for  20220329\n",
      "Downloading snapshot: http://web.archive.org/web/20220329235125/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220329235125\n",
      "Downloaded and saved: apple.com/20220329235125.html\n",
      "Attempt 1: Getting snapshot for 20220227\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {}, 'timestamp': '20220227'}  for  20220227\n",
      "No snapshot available apple.com : 20220227\n",
      "Attempt 1: Getting snapshot for 20220128\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20220129000736/https://www.apple.com/', 'timestamp': '20220129000736'}}, 'timestamp': '20220128'}  for  20220128\n",
      "Downloading snapshot: http://web.archive.org/web/20220129000736/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20220129000736\n",
      "Downloaded and saved: apple.com/20220129000736.html\n",
      "Attempt 1: Getting snapshot for 20211229\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20211229235326/https://www.apple.com/', 'timestamp': '20211229235326'}}, 'timestamp': '20211229'}  for  20211229\n",
      "Downloading snapshot: http://web.archive.org/web/20211229235326/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20211229235326\n",
      "Downloaded and saved: apple.com/20211229235326.html\n",
      "Attempt 1: Getting snapshot for 20211129\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20211130000006/https://www.apple.com/', 'timestamp': '20211130000006'}}, 'timestamp': '20211129'}  for  20211129\n",
      "Downloading snapshot: http://web.archive.org/web/20211130000006/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20211130000006\n",
      "Downloaded and saved: apple.com/20211130000006.html\n",
      "Attempt 1: Getting snapshot for 20211030\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20211031000219/https://www.apple.com/', 'timestamp': '20211031000219'}}, 'timestamp': '20211030'}  for  20211030\n",
      "Downloading snapshot: http://web.archive.org/web/20211031000219/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20211031000219\n",
      "Downloaded and saved: apple.com/20211031000219.html\n",
      "Attempt 1: Getting snapshot for 20210930\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210930235956/https://www.apple.com/', 'timestamp': '20210930235956'}}, 'timestamp': '20210930'}  for  20210930\n",
      "Downloading snapshot: http://web.archive.org/web/20210930235956/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210930235956\n",
      "Error downloading http://web.archive.org/web/20210930235956/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20210930235956/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a14c9da10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20210930235956\n",
      "Downloaded and saved: apple.com/20210930235956.html\n",
      "Attempt 1: Getting snapshot for 20210831\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210831234227/https://www.apple.com/', 'timestamp': '20210831234227'}}, 'timestamp': '20210831'}  for  20210831\n",
      "Downloading snapshot: http://web.archive.org/web/20210831234227/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210831234227\n",
      "Error downloading http://web.archive.org/web/20210831234227/https://www.apple.com/: HTTPConnectionPool(host='web.archive.org', port=80): Max retries exceeded with url: /web/20210831234227/https://www.apple.com/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x744a13c579d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Attempt 2: Getting snapshot for 20210831234227\n",
      "Downloaded and saved: apple.com/20210831234227.html\n",
      "Attempt 1: Getting snapshot for 20210801\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210802000516/http://www.apple.com/', 'timestamp': '20210802000516'}}, 'timestamp': '20210801'}  for  20210801\n",
      "Downloading snapshot: http://web.archive.org/web/20210802000516/http://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210802000516\n",
      "Downloaded and saved: apple.com/20210802000516.html\n",
      "Attempt 1: Getting snapshot for 20210702\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210702235419/https://www.apple.com/', 'timestamp': '20210702235419'}}, 'timestamp': '20210702'}  for  20210702\n",
      "Downloading snapshot: http://web.archive.org/web/20210702235419/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210702235419\n",
      "Downloaded and saved: apple.com/20210702235419.html\n",
      "Attempt 1: Getting snapshot for 20210602\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210603001112/https://www.apple.com/', 'timestamp': '20210603001112'}}, 'timestamp': '20210602'}  for  20210602\n",
      "Downloading snapshot: http://web.archive.org/web/20210603001112/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210603001112\n",
      "Downloaded and saved: apple.com/20210603001112.html\n",
      "Attempt 1: Getting snapshot for 20210503\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210504001429/https://www.apple.com/', 'timestamp': '20210504001429'}}, 'timestamp': '20210503'}  for  20210503\n",
      "Downloading snapshot: http://web.archive.org/web/20210504001429/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210504001429\n",
      "Downloaded and saved: apple.com/20210504001429.html\n",
      "Attempt 1: Getting snapshot for 20210403\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210404005558/http://www.apple.com/', 'timestamp': '20210404005558'}}, 'timestamp': '20210403'}  for  20210403\n",
      "Downloading snapshot: http://web.archive.org/web/20210404005558/http://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210404005558\n",
      "Downloaded and saved: apple.com/20210404005558.html\n",
      "Attempt 1: Getting snapshot for 20210304\n",
      "Check:  {'url': 'apple.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20210305000001/https://www.apple.com/', 'timestamp': '20210305000001'}}, 'timestamp': '20210304'}  for  20210304\n",
      "Downloading snapshot: http://web.archive.org/web/20210305000001/https://www.apple.com/\n",
      "Attempt 1: Getting snapshot for 20210305000001\n",
      "Downloaded and saved: apple.com/20210305000001.html\n",
      "Attempt 1: Getting snapshot for 20210202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m progress_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraping_progress.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     97\u001b[0m urls \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranco_top_10k.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebsite\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m---> 99\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcrawlWayback\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Cell \u001b[0;32mIn[27], line 83\u001b[0m, in \u001b[0;36mcrawlWayback\u001b[0;34m(urls, progress_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwebsite\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, already processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mdownload_past_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebsite\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Save progress after each URL is processed\u001b[39;00m\n\u001b[1;32m     85\u001b[0m record \u001b[38;5;241m=\u001b[39m [website,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m, in \u001b[0;36mdownload_past_versions\u001b[0;34m(website, max_retries)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m): \u001b[38;5;66;03m#set to 60 later for going back every month for 5 years\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     past_date \u001b[38;5;241m=\u001b[39m (current_date \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#set days i*30 for 1 month\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     snapshot \u001b[38;5;241m=\u001b[39m \u001b[43mget_wayback_snapshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebsite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m snapshot:\n\u001b[1;32m     26\u001b[0m         wayback_url \u001b[38;5;241m=\u001b[39m snapshot[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m, in \u001b[0;36mget_wayback_snapshot\u001b[0;34m(website, date, max_retries)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Getting snapshot for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:395\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    393\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:234\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[1;32m    233\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1296\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1344\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1293\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1052\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1050\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1052\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:990\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 990\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:169\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py:86\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     85\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 86\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_wayback_snapshot(website, date, max_retries=5):\n",
    "    url = f\"http://archive.org/wayback/available?url={website}&timestamp={date}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}: Getting snapshot for {date}\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            print(\"Check: \",data, \" for \",date)\n",
    "            if 'archived_snapshots' in data and data['archived_snapshots']:\n",
    "                return (data['archived_snapshots']['closest']['url'],data['archived_snapshots']['closest']['timestamp'])\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error: {e}. Retrying in 3 seconds...\")\n",
    "            time.sleep(5)\n",
    "    print(f\"Failed to get snapshot for {date} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def download_past_versions(website, max_retries = 10):\n",
    "    current_date = datetime.now()\n",
    "    for i in range(60): #set to 60 later for going back every month for 5 years\n",
    "        past_date = (current_date - timedelta(days=i*30)).strftime('%Y%m%d') #set days i*30 for 1 month\n",
    "        snapshot = get_wayback_snapshot(website, past_date)\n",
    "        if snapshot:\n",
    "            wayback_url = snapshot[0]\n",
    "            wayback_timestamp = snapshot[1]\n",
    "            print(f\"Downloading snapshot: {wayback_url}\")\n",
    "\n",
    "            if (not os.path.exists(website)):\n",
    "                os.makedirs(website, exist_ok=True)\n",
    "\n",
    "            filename = f\"{wayback_timestamp}.html\"\n",
    "            filepath = os.path.join(website,filename)\n",
    "        \n",
    "            for attempt in range(max_retries):\n",
    "                print(f\"Attempt {attempt + 1}: Getting snapshot for {wayback_timestamp}\")\n",
    "            \n",
    "                try:\n",
    "                    time.sleep(3)\n",
    "                    response = requests.get(wayback_url)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(response.text)\n",
    "                        print(f\"Downloaded and saved: {filepath}\")\n",
    "                        break\n",
    "                    elif response.status_code == 404:\n",
    "                        print(f\"File not found (404): {wayback_url}. Skipping...\")\n",
    "                        break  # Stop retrying on 404 errors since it just doesn't exist\n",
    "                    else:\n",
    "                        print(f\"Failed to download {wayback_url}: {response.status_code}\")\n",
    "                        time.sleep(5) #wait before retrying\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error downloading {wayback_url}: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        else:\n",
    "            print(f\"No snapshot available {website} : {past_date}\")\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Website Name\", \"Completed\"])\n",
    "\n",
    "def save_progress(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def crawlWayback(urls, progress_file):\n",
    "    # Load previous progress if any\n",
    "    progress_df = load_progress(progress_file)\n",
    "    completed_urls = progress_df['Website Name'].tolist()\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    for idx, website in enumerate(urls):\n",
    "        if website in completed_urls:\n",
    "            print(f\"Skipping {website}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        download_past_versions(website)    \n",
    "        # Save progress after each URL is processed\n",
    "        record = [website,\"completed\"]\n",
    "        progress_df.loc[len(progress_df)] = record\n",
    "\n",
    "        save_progress(progress_df, progress_file)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed {idx + 1}/{total_urls}: {website}\")\n",
    "\n",
    "    return progress_df\n",
    "\n",
    "\n",
    "progress_file = 'scraping_progress.csv'\n",
    "urls = pd.read_csv('tranco_top_10k.csv')['website'].to_list()\n",
    "\n",
    "df = crawlWayback(urls, progress_file)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 saved with rows 0 to 2499\n",
      "Part 2 saved with rows 2500 to 4999\n",
      "Part 3 saved with rows 5000 to 7499\n",
      "Part 4 saved with rows 7500 to 9999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_dataframe(df, num_splits):\n",
    "    chunk_size = len(df) // num_splits\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        # Calculate the start and end index for each chunk\n",
    "        start_idx = i * chunk_size\n",
    "        # Make sure the last chunk includes any leftover rows\n",
    "        if i == num_splits - 1:\n",
    "            end_idx = len(df)\n",
    "        else:\n",
    "            end_idx = (i + 1) * chunk_size\n",
    "        \n",
    "        chunk_df = df.iloc[start_idx:end_idx]\n",
    "        chunk_df.to_csv(f'tranco_10k_{i+1}.csv', index=False)\n",
    "\n",
    "        print(f'Part {i+1} saved with rows {start_idx} to {end_idx-1}')\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv('tranco_top_10k.csv')\n",
    "split_dataframe(df, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling using requests library and CDX API for valid results - requests library does not load the webpage entirely before downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190916 20240914\n",
      "Fetching for googleapis.com\n",
      "Here:  <Response [200]>\n",
      "No snapshots available for googleapis.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# CDX API to get snapshots for the past 5 years, month by month\n",
    "def get_wayback_snapshots_cdx(website, start_date, end_date):\n",
    "    print(f\"Fetching for {website}\")\n",
    "    url = f\"http://web.archive.org/cdx/search/cdx?url={website}&from={start_date}&to={end_date}&filter=statuscode:200&output=json&collapse=timestamp:6&limit=61\" # Collapse to one per month\n",
    "    response = requests.get(url)\n",
    "    print(\"Here: \",response)\n",
    "    \n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        snapshots = response.json()\n",
    "        if len(snapshots) > 1:  # First item in snapshots is the header\n",
    "            return snapshots[1:]  # Skip the header\n",
    "    return None\n",
    "\n",
    "# Function to download and save the archived snapshots\n",
    "def download_webpage(snapshot, website_folder):\n",
    "    timestamp = snapshot[1]\n",
    "    wayback_url = f\"http://web.archive.org/web/{timestamp}/{snapshot[2]}\"\n",
    "    print(f\"Downloading {wayback_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(wayback_url)\n",
    "        time.sleep\n",
    "        if response.status_code == 200:\n",
    "            # Save the webpage with timestamp\n",
    "            filename = f\"{timestamp}.html\"\n",
    "            filepath = os.path.join(website_folder, filename)\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            print(f\"Saved {filepath}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {wayback_url}: Status {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {wayback_url}: {e}\")\n",
    "\n",
    "# Main function to process website over 5 years and download monthly snapshots\n",
    "def download_past_versions_cdx(website):\n",
    "    # Calculate 5 years back\n",
    "    end_date = datetime.now().strftime('%Y%m%d')\n",
    "    start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y%m%d')\n",
    "\n",
    "    # Fetch all snapshots month by month using CDX API\n",
    "    print(start_date,end_date)\n",
    "    snapshots = get_wayback_snapshots_cdx(website, start_date, end_date)\n",
    "    \n",
    "    if snapshots:\n",
    "        print(f\"Found {len(snapshots)} snapshots for {website}\")\n",
    "        \n",
    "        # Create a folder to store snapshots\n",
    "        if not os.path.exists(website):\n",
    "            os.makedirs(website)\n",
    "        \n",
    "        # Loop through each snapshot and download it\n",
    "        for snapshot in snapshots:\n",
    "            download_webpage(snapshot, website)\n",
    "    else:\n",
    "        print(f\"No snapshots available for {website}\")\n",
    "\n",
    "# Example usage\n",
    "# urls = pd.read_csv('tranco_top_10k.csv')['website'].to_list()\n",
    "urls = [\"googleapis.com\"]\n",
    "for url in urls:\n",
    "    download_past_versions_cdx(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling using CDX API and Selenium - works perfectly but slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping google.com, already processed.\n",
      "Skipping amazonaws.com, already processed.\n",
      "Skipping microsoft.com, already processed.\n",
      "Generated all snapshots\n",
      "Downloaded and saved: facebook.com/20190916002521.html\n",
      "Downloaded and saved: facebook.com/20191001000810.html\n",
      "Downloaded and saved: facebook.com/20191101002347.html\n",
      "Downloaded and saved: facebook.com/20191201000258.html\n",
      "Downloaded and saved: facebook.com/20200101000525.html\n",
      "Downloaded and saved: facebook.com/20200201000203.html\n",
      "Downloaded and saved: facebook.com/20200301000302.html\n",
      "Downloaded and saved: facebook.com/20200401000113.html\n",
      "Downloaded and saved: facebook.com/20200501000250.html\n",
      "Downloaded and saved: facebook.com/20200623000139.html\n",
      "TimeoutException on attempt 1 for https://web.archive.org/web/20200701000026/https://www.facebook.com/: Message: timeout: Timed out receiving message from renderer: 295.217\n",
      "  (Session info: chrome-headless-shell=122.0.6261.111)\n",
      "Stacktrace:\n",
      "#0 0x561eec45af33 <unknown>\n",
      "#1 0x561eec152ce6 <unknown>\n",
      "#2 0x561eec13a071 <unknown>\n",
      "#3 0x561eec139db3 <unknown>\n",
      "#4 0x561eec138814 <unknown>\n",
      "#5 0x561eec138f1f <unknown>\n",
      "#6 0x561eec149185 <unknown>\n",
      "#7 0x561eec15e8ac <unknown>\n",
      "#8 0x561eec163e8b <unknown>\n",
      "#9 0x561eec1395ae <unknown>\n",
      "#10 0x561eec15e624 <unknown>\n",
      "#11 0x561eec1debf1 <unknown>\n",
      "#12 0x561eec1bfc53 <unknown>\n",
      "#13 0x561eec190db3 <unknown>\n",
      "#14 0x561eec19177e <unknown>\n",
      "#15 0x561eec42086b <unknown>\n",
      "#16 0x561eec424885 <unknown>\n",
      "#17 0x561eec40e181 <unknown>\n",
      "#18 0x561eec425412 <unknown>\n",
      "#19 0x561eec3f225f <unknown>\n",
      "#20 0x561eec449528 <unknown>\n",
      "#21 0x561eec449723 <unknown>\n",
      "#22 0x561eec45a0e4 <unknown>\n",
      "#23 0x7e26d3894ac3 <unknown>\n",
      "\n",
      "Downloaded and saved: facebook.com/20200801001211.html\n",
      "Downloaded and saved: facebook.com/20200901000026.html\n",
      "TimeoutException on attempt 1 for https://web.archive.org/web/20201001000143/https://www.facebook.com/: Message: timeout: Timed out receiving message from renderer: 296.051\n",
      "  (Session info: chrome-headless-shell=122.0.6261.111)\n",
      "Stacktrace:\n",
      "#0 0x561eec45af33 <unknown>\n",
      "#1 0x561eec152ce6 <unknown>\n",
      "#2 0x561eec13a071 <unknown>\n",
      "#3 0x561eec139db3 <unknown>\n",
      "#4 0x561eec138814 <unknown>\n",
      "#5 0x561eec138f1f <unknown>\n",
      "#6 0x561eec149185 <unknown>\n",
      "#7 0x561eec15e8ac <unknown>\n",
      "#8 0x561eec163e8b <unknown>\n",
      "#9 0x561eec1395ae <unknown>\n",
      "#10 0x561eec15e624 <unknown>\n",
      "#11 0x561eec1debf1 <unknown>\n",
      "#12 0x561eec1bfc53 <unknown>\n",
      "#13 0x561eec190db3 <unknown>\n",
      "#14 0x561eec19177e <unknown>\n",
      "#15 0x561eec42086b <unknown>\n",
      "#16 0x561eec424885 <unknown>\n",
      "#17 0x561eec40e181 <unknown>\n",
      "#18 0x561eec425412 <unknown>\n",
      "#19 0x561eec3f225f <unknown>\n",
      "#20 0x561eec449528 <unknown>\n",
      "#21 0x561eec449723 <unknown>\n",
      "#22 0x561eec45a0e4 <unknown>\n",
      "#23 0x7e26d3894ac3 <unknown>\n",
      "\n",
      "Downloaded and saved: facebook.com/20201101000009.html\n",
      "Downloaded and saved: facebook.com/20201213000051.html\n",
      "Downloaded and saved: facebook.com/20210101000246.html\n",
      "Downloaded and saved: facebook.com/20210201000121.html\n",
      "Downloaded and saved: facebook.com/20210301000110.html\n",
      "Downloaded and saved: facebook.com/20210401000455.html\n",
      "Downloaded and saved: facebook.com/20210501000016.html\n",
      "Downloaded and saved: facebook.com/20210601000359.html\n",
      "Downloaded and saved: facebook.com/20210703000048.html\n",
      "Downloaded and saved: facebook.com/20210801000220.html\n",
      "Downloaded and saved: facebook.com/20210901000835.html\n",
      "Downloaded and saved: facebook.com/20211001002859.html\n",
      "Downloaded and saved: facebook.com/20211101000430.html\n",
      "Downloaded and saved: facebook.com/20211201000018.html\n",
      "Downloaded and saved: facebook.com/20220101000053.html\n",
      "Downloaded and saved: facebook.com/20220201000111.html\n",
      "Downloaded and saved: facebook.com/20220301000150.html\n",
      "Downloaded and saved: facebook.com/20220401000133.html\n",
      "Downloaded and saved: facebook.com/20220526000253.html\n",
      "Downloaded and saved: facebook.com/20220601000007.html\n",
      "Downloaded and saved: facebook.com/20220701000009.html\n",
      "Downloaded and saved: facebook.com/20220801000530.html\n",
      "Downloaded and saved: facebook.com/20220901000009.html\n",
      "Downloaded and saved: facebook.com/20221001000022.html\n",
      "Downloaded and saved: facebook.com/20221101000035.html\n",
      "Downloaded and saved: facebook.com/20221212000048.html\n",
      "Downloaded and saved: facebook.com/20230101000026.html\n",
      "Downloaded and saved: facebook.com/20230201000027.html\n",
      "Downloaded and saved: facebook.com/20230304000143.html\n",
      "Downloaded and saved: facebook.com/20230401000029.html\n",
      "Downloaded and saved: facebook.com/20230501000707.html\n",
      "Downloaded and saved: facebook.com/20230629000030.html\n",
      "Downloaded and saved: facebook.com/20230701000104.html\n",
      "Downloaded and saved: facebook.com/20230801000216.html\n",
      "Downloaded and saved: facebook.com/20230901000054.html\n",
      "Downloaded and saved: facebook.com/20231030000612.html\n",
      "Downloaded and saved: facebook.com/20231101000017.html\n",
      "Downloaded and saved: facebook.com/20231201000106.html\n",
      "Downloaded and saved: facebook.com/20240101000001.html\n",
      "Downloaded and saved: facebook.com/20240210000110.html\n",
      "Downloaded and saved: facebook.com/20240301000433.html\n",
      "Downloaded and saved: facebook.com/20240401000309.html\n",
      "Downloaded and saved: facebook.com/20240501000331.html\n",
      "Downloaded and saved: facebook.com/20240621000045.html\n",
      "Downloaded and saved: facebook.com/20240701000138.html\n",
      "Downloaded and saved: facebook.com/20240801000042.html\n",
      "Downloaded and saved: facebook.com/20240901000408.html\n",
      "Processed 4/10000: facebook.com\n"
     ]
    }
   ],
   "source": [
    "def add_month(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    new_date_obj = date_obj + relativedelta(months=1)\n",
    "    new_date_str = new_date_obj.strftime('%Y%m%d')\n",
    "    return new_date_str\n",
    "\n",
    "# Function to get snapshots with pagination\n",
    "def get_all_snapshots(url, start_date, end_date, limit=100000):\n",
    "    all_snapshots = []\n",
    "    current_start = start_date\n",
    "    while current_start <= end_date:\n",
    "        api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&from={current_start}&to={end_date}&output=json&limit={limit}\"\n",
    "        for i in range(5): #max tries are 5\n",
    "            try:\n",
    "                response = requests.get(api_url)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    snapshots = data[1:]  # Excluding the header\n",
    "                    if not snapshots:\n",
    "                        print(\"No snapshots to fetch\")\n",
    "                        current_start = add_month(end_date)\n",
    "                        break\n",
    "                    monthly_end = snapshots[-1][1][:8]  # Use the timestamp of the last snapshot to set the new start date\n",
    "                    if monthly_end !=current_start:\n",
    "                        current_start = monthly_end\n",
    "\n",
    "                    all_snapshots.append(filter_snapshots_by_month(snapshots))\n",
    "                    break\n",
    "            \n",
    "                else:\n",
    "                    print(\"Failed to fetch data\")\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "        current_start = add_month(current_start)\n",
    "\n",
    "        \n",
    "    return all_snapshots\n",
    "\n",
    "# Function to filter snapshots to one per month\n",
    "def filter_snapshots_by_month(snapshots):\n",
    "    snapshots_by_month = {}\n",
    "    \n",
    "    for snapshot in snapshots:\n",
    "        timestamp = snapshot[1]\n",
    "        date_str = timestamp[:6]  # Extract YYYYMM\n",
    "        if date_str not in snapshots_by_month:\n",
    "            snapshots_by_month[date_str] = snapshot\n",
    "    \n",
    "    return snapshots_by_month\n",
    "\n",
    "\n",
    "# Function to generate a date range\n",
    "def generate_date_range(years=5):\n",
    "    current_date = datetime.now()\n",
    "    start_date = (current_date - timedelta(days=years*365)).strftime('%Y%m%d')\n",
    "    end_date = current_date.strftime('%Y%m%d')\n",
    "    return start_date, end_date\n",
    "\n",
    "def generateAllSnapshots(url):\n",
    "    start_date, end_date = generate_date_range()\n",
    "    snapshots = get_all_snapshots(url, start_date, end_date, limit=100000)\n",
    "    all_snapshots = []\n",
    "    for snap in snapshots:\n",
    "        for month, snapshot in snap.items():\n",
    "            all_snapshots.append(snapshot)\n",
    "    return all_snapshots\n",
    "\n",
    "\n",
    "def download_past_versions(website,driver ,max_retries = 5):\n",
    "    # print(f\"Processing {website}\")\n",
    "    wayback_base_url = \"https://web.archive.org/web/\"\n",
    "\n",
    "    snapshots = generateAllSnapshots(website)\n",
    "    print(\"Generated all snapshots\")\n",
    "    if snapshots:\n",
    "        for snapshot in snapshots:\n",
    "\n",
    "            timestamp = snapshot[1]\n",
    "            original_url = snapshot[2]\n",
    "            wayback_url = f\"{wayback_base_url}{timestamp}/{original_url}\" #A resource at the wayback has a url of this format\n",
    "            wayback_timestamp = snapshot[1]\n",
    "            # print(f\"Downloading snapshot: {wayback_url}\")\n",
    "\n",
    "            if (not os.path.exists(website)):\n",
    "                os.makedirs(website, exist_ok=True)\n",
    "\n",
    "            filename = f\"{wayback_timestamp}.html\"\n",
    "            filepath = os.path.join(website,filename)\n",
    "        \n",
    "            for attempt in range(max_retries):\n",
    "                # print(f\"Attempt {attempt + 1}: Getting snapshot for {wayback_timestamp}\")\n",
    "            \n",
    "                try:\n",
    "                    driver.get(wayback_url)\n",
    "                    page_source = driver.page_source\n",
    "\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(page_source)\n",
    "                    print(f\"Downloaded and saved: {filepath}\")\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error downloading {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "                except TimeoutException as e:\n",
    "                    print(f\"TimeoutException on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)  # Retry after a delay\n",
    "                    break\n",
    "                except WebDriverException as e:\n",
    "                    print(f\"WebDriverException on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "                except Exception as e:\n",
    "                    print(f\"General error on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "    \n",
    "\n",
    "    else:\n",
    "        print(f\"No snapshots available for {website}\")\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Website Name\", \"Completed\"])\n",
    "\n",
    "def save_progress(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def crawlWayback(urls, progress_file):\n",
    "    # Load previous progress if any\n",
    "    progress_df = load_progress(progress_file)\n",
    "    completed_urls = progress_df['Website Name'].tolist()\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    for idx, website in enumerate(urls):\n",
    "        if website in completed_urls:\n",
    "            print(f\"Skipping {website}, already processed.\")\n",
    "            continue\n",
    "        print(\"Processing \",website)\n",
    "        \n",
    "        download_past_versions(website,driver)    \n",
    "        # Save progress after each URL is processed\n",
    "        record = [website,\"completed\"]\n",
    "        progress_df.loc[len(progress_df)] = record\n",
    "\n",
    "        save_progress(progress_df, progress_file)\n",
    "        # Print progress\n",
    "        print(f\"Processed {idx + 1}/{total_urls}: {website}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return progress_df\n",
    "\n",
    "urls = pd.read_csv('tranco_top_10k.csv')['website'].to_list()\n",
    "progress_file = \"progress.csv\"\n",
    "crawlWayback(urls,progress_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping google.com, already processed.\n",
      "Skipping amazonaws.com, already processed.\n",
      "Skipping microsoft.com, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/json/decoder.py:353: RuntimeWarning: coroutine 'crawlWayback' was never awaited\n",
      "  obj, end = self.scan_once(s, idx)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated all snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Chromium download.\n",
      "100%|██████████| 183M/183M [01:21<00:00, 2.25Mb/s] \n",
      "[INFO] Beginning extraction\n",
      "[INFO] Chromium extracted to: /home/abdullah/.local/share/pyppeteer/local-chromium/1181205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: facebook.com/20230915000216.html\n",
      "Downloaded and saved: facebook.com/20231001000730.html\n",
      "Downloaded and saved: facebook.com/20231101000017.html\n",
      "Downloaded and saved: facebook.com/20231201000106.html\n",
      "General error on attempt 1 for https://web.archive.org/web/20240129000310/https://www.facebook.com/: net::ERR_HTTP2_SERVER_REFUSED_STREAM at https://web.archive.org/web/20240129000310/https://www.facebook.com/\n",
      "Downloaded and saved: facebook.com/20240129000310.html\n",
      "Downloaded and saved: facebook.com/20240201000041.html\n",
      "Downloaded and saved: facebook.com/20240301000433.html\n",
      "Downloaded and saved: facebook.com/20240401000309.html\n",
      "Downloaded and saved: facebook.com/20240501000331.html\n",
      "General error on attempt 1 for https://web.archive.org/web/20240615000050/https://www.facebook.com/: net::ERR_HTTP2_SERVER_REFUSED_STREAM at https://web.archive.org/web/20240615000050/https://www.facebook.com/\n",
      "Downloaded and saved: facebook.com/20240615000050.html\n",
      "Downloaded and saved: facebook.com/20240701000138.html\n",
      "General error on attempt 1 for https://web.archive.org/web/20240801000042/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 2 for https://web.archive.org/web/20240801000042/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 3 for https://web.archive.org/web/20240801000042/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 4 for https://web.archive.org/web/20240801000042/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 5 for https://web.archive.org/web/20240801000042/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 1 for https://web.archive.org/web/20240901000408/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 2 for https://web.archive.org/web/20240901000408/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 3 for https://web.archive.org/web/20240901000408/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 4 for https://web.archive.org/web/20240901000408/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "General error on attempt 5 for https://web.archive.org/web/20240901000408/http://facebook.com/: Unable to render the page. Try increasing timeout\n",
      "Processed 4/10000: facebook.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21783/124501437.py:37: RuntimeWarning: coroutine 'Launcher.killChrome' was never awaited\n",
      "  continue\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "import asyncio\n",
    "\n",
    "def add_month(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    new_date_obj = date_obj + relativedelta(months=1)\n",
    "    new_date_str = new_date_obj.strftime('%Y%m%d')\n",
    "    return new_date_str\n",
    "\n",
    "# Function to get snapshots with pagination\n",
    "def get_all_snapshots(url, start_date, end_date, limit=100000):\n",
    "    all_snapshots = []\n",
    "    current_start = start_date\n",
    "\n",
    "    while current_start <= end_date:\n",
    "        api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&from={current_start}&to={end_date}&output=json&limit={limit}\"\n",
    "        for i in range(5): #max tries are 5\n",
    "            try:\n",
    "                response = requests.get(api_url)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    snapshots = data[1:]  # Excluding the header\n",
    "                    if not snapshots:\n",
    "                        break\n",
    "                    monthly_end = snapshots[-1][1][:8]  # Use the timestamp of the last snapshot to set the new start date\n",
    "                    if monthly_end !=current_start:\n",
    "                        current_start = monthly_end\n",
    "                    current_start = add_month(current_start)\n",
    "\n",
    "                    all_snapshots.append(filter_snapshots_by_month(snapshots))\n",
    "                    break\n",
    "            \n",
    "                else:\n",
    "                    print(\"Failed to fetch data\")\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return all_snapshots\n",
    "\n",
    "# Function to filter snapshots to one per month\n",
    "def filter_snapshots_by_month(snapshots):\n",
    "    snapshots_by_month = {}\n",
    "    \n",
    "    for snapshot in snapshots:\n",
    "        timestamp = snapshot[1]\n",
    "        date_str = timestamp[:6]  # Extract YYYYMM\n",
    "        if date_str not in snapshots_by_month:\n",
    "            snapshots_by_month[date_str] = snapshot\n",
    "    \n",
    "    return snapshots_by_month\n",
    "\n",
    "\n",
    "# Function to generate a date range\n",
    "def generate_date_range(years=5):\n",
    "    current_date = datetime.now()\n",
    "    start_date = (current_date - timedelta(days=years*365)).strftime('%Y%m%d')\n",
    "    end_date = current_date.strftime('%Y%m%d')\n",
    "    return start_date, end_date\n",
    "\n",
    "def generateAllSnapshots(url):\n",
    "    start_date, end_date = generate_date_range()\n",
    "    snapshots = get_all_snapshots(url, start_date, end_date, limit=100000)\n",
    "    all_snapshots = []\n",
    "    for snap in snapshots:\n",
    "        for month, snapshot in snap.items():\n",
    "            all_snapshots.append(snapshot)\n",
    "    return all_snapshots\n",
    "\n",
    "\n",
    "async def download_past_versions(website,session ,max_retries = 5):\n",
    "    # print(f\"Processing {website}\")\n",
    "    wayback_base_url = \"https://web.archive.org/web/\"\n",
    "\n",
    "    snapshots = generateAllSnapshots(website)\n",
    "    print(\"Generated all snapshots\")\n",
    "    if snapshots:\n",
    "        for snapshot in snapshots:\n",
    "\n",
    "            timestamp = snapshot[1]\n",
    "            original_url = snapshot[2]\n",
    "            wayback_url = f\"{wayback_base_url}{timestamp}/{original_url}\" #A resource at the wayback has a url of this format\n",
    "            wayback_timestamp = snapshot[1]\n",
    "            # print(f\"Downloading snapshot: {wayback_url}\")\n",
    "\n",
    "            if (not os.path.exists(\"experimnt-2\")):\n",
    "                os.makedirs(f\"expriment-2 {website}\", exist_ok=True)\n",
    "\n",
    "            filename = f\"{wayback_timestamp}.html\"\n",
    "            filepath = os.path.join(website,filename)\n",
    "        \n",
    "            for attempt in range(max_retries):\n",
    "                # print(f\"Attempt {attempt + 1}: Getting snapshot for {wayback_timestamp}\")\n",
    "            \n",
    "                try:\n",
    "                    response = await session.get(wayback_url)\n",
    "                    await response.html.arender(timeout=10)\n",
    "\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(response.html.html)\n",
    "                    print(f\"Downloaded and saved: {filepath}\")\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error downloading {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "                except TimeoutException as e:\n",
    "                    print(f\"TimeoutException on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)  # Retry after a delay\n",
    "                except WebDriverException as e:\n",
    "                    print(f\"WebDriverException on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "                except Exception as e:\n",
    "                    print(f\"General error on attempt {attempt + 1} for {wayback_url}: {e}\")\n",
    "                    time.sleep(5)\n",
    "    \n",
    "\n",
    "    else:\n",
    "        print(f\"No snapshots available for {website}\")\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def load_progress(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Website Name\", \"Completed\"])\n",
    "\n",
    "def save_progress(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "async def crawlWayback(urls, progress_file):\n",
    "    # Load previous progress if any\n",
    "    session = AsyncHTMLSession()\n",
    "    progress_df = load_progress(progress_file)\n",
    "    completed_urls = progress_df['Website Name'].tolist()\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    for idx, website in enumerate(urls):\n",
    "        if website in completed_urls:\n",
    "            print(f\"Skipping {website}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        await download_past_versions(website,session)    \n",
    "        # Save progress after each URL is processed\n",
    "        record = [website,\"completed\"]\n",
    "        progress_df.loc[len(progress_df)] = record\n",
    "\n",
    "        save_progress(progress_df, progress_file)\n",
    "        # Print progress\n",
    "        print(f\"Processed {idx + 1}/{total_urls}: {website}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return progress_df\n",
    "\n",
    "async def main():\n",
    "    urls = pd.read_csv('tranco_top_10k.csv')['website'].to_list()\n",
    "    filepath = 'progress.csv'\n",
    "    await crawlWayback(urls, filepath)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if an event loop is running, and use it if necessary\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:  # No event loop running\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        # If there is a running loop, use 'await' instead of 'asyncio.run()'\n",
    "        await main()\n",
    "    else:\n",
    "        asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
